{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7280d409",
   "metadata": {},
   "source": [
    "### 1. 5 diferenças entre o AdaBoost e o GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c644f0",
   "metadata": {},
   "source": [
    "**Método de combinação:** <br> \n",
    "O AdaBoost combina várias versões de um único modelo fraco, os chamados stumps. O GBM, por outro lado, cria modelos de árvore de decisão.\n",
    "\n",
    "**Pesos das amostras** <br> \n",
    "No AdaBoost, as amostras são ponderadas de acordo com sua dificuldade de classificação. As amostras mal classificadas recebem um peso maior, permitindo que os modelos subsequentes se concentrem nelas. No GBM, não há ponderação de amostras, pois o foco está na redução dos erros residuais.\n",
    "\n",
    "**Abordagem de otimização** <br> \n",
    "O AdaBoost utiliza uma abordagem de otimização por gradiente descendente, onde o objetivo é minimizar a função de perda ajustando os pesos dos modelos fracos. O GBM também utiliza uma abordagem de otimização por gradiente descendente, mas em vez de ajustar os pesos, ele ajusta os parâmetros dos modelos de árvore de decisão.\n",
    "\n",
    "**Maneira de criar os modelos** <br> \n",
    "No AdaBoost, os modelos fracos são criados sequencialmente, onde cada novo modelo é treinado para corrigir os erros cometidos pelos modelos anteriores. No GBM, os modelos de árvore de decisão são criados de forma iterativa, onde cada nova árvore é treinada para minimizar os erros residuais.\n",
    "\n",
    "**Robustez a ruídos** <br> \n",
    "O AdaBoost é sensível a ruídos e outliers, pois os modelos subsequentes se concentram em corrigir os erros cometidos pelos modelos anteriores. O GBM é mais robusto a ruídos e outliers, pois a abordagem de otimização por gradiente descendente ajuda a reduzir o impacto desses valores atípicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf438b7",
   "metadata": {},
   "source": [
    "### 2. Exemplo do GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c6b12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=1, \n",
    "    random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e901e54",
   "metadata": {},
   "source": [
    "### 3. Hiperparâmetros importantes no GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5fdbbb",
   "metadata": {},
   "source": [
    "**n_estimators:** <br> \n",
    "Este parâmetro define o número de árvores de decisão que serão criadas. Um valor maior para n_estimators pode aumentar a capacidade do modelo, mas também pode levar a um maior tempo de treinamento. \n",
    "\n",
    "**learning_rate** <br> \n",
    "O learning_rate controla a taxa de aprendizado do algoritmo. Um valor menor para learning_rate reduz a contribuição de cada árvore, tornando o processo de aprendizado mais lento, mas potencialmente mais preciso. Por outro lado, um valor maior para learning_rate aumenta a contribuição de cada árvore, tornando o processo de aprendizado mais rápido, mas potencialmente menos preciso.\n",
    "\n",
    "**max_depth** <br> \n",
    "O max_depth define a profundidade máxima de cada árvore de decisão. Um valor maior para max_depth permite que as árvores sejam mais complexas e capazes de se ajustar aos dados de treinamento com mais detalhes. No entanto, um valor muito alto pode levar a um overfitting. \n",
    "\n",
    "**min_samples_split** <br> \n",
    "O min_samples_split define o número mínimo de amostras necessárias para dividir um nó interno durante a construção de uma árvore. Um valor maior para min_samples_split pode evitar o overfitting, pois requer um número mínimo de amostras em um nó para que a divisão ocorra. No entanto, um valor muito alto pode levar a uma árvore que não captura informações importantes nos dados. \n",
    "\n",
    "**loss** <br> \n",
    "O loss define a função de perda utilizada para otimizar o modelo. O GradientBoostingRegressor suporta várias funções de perda, como squared_error, absolute_error, huber e quantile. A escolha da função de perda depende do problema específico e das características dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce79d9",
   "metadata": {},
   "source": [
    "### 4. Uso do GridSearchCV para encontrar os melhores hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789cd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b482dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [100, 150, 200, 250],\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'max_depth': [1, 2],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "    'random_state': [0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfec08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 110 ms, total: 1.17 s\n",
      "Wall time: 43.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.2, 0.3],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;, &#x27;huber&#x27;,\n",
       "                                  &#x27;quantile&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [1, 2], &#x27;min_samples_split&#x27;: [2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150, 200, 250],\n",
       "                         &#x27;random_state&#x27;: [0]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.2, 0.3],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;, &#x27;huber&#x27;,\n",
       "                                  &#x27;quantile&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [1, 2], &#x27;min_samples_split&#x27;: [2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150, 200, 250],\n",
       "                         &#x27;random_state&#x27;: [0]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.2, 0.3],\n",
       "                         'loss': ['squared_error', 'absolute_error', 'huber',\n",
       "                                  'quantile'],\n",
       "                         'max_depth': [1, 2], 'min_samples_split': [2, 3],\n",
       "                         'n_estimators': [100, 150, 200, 250],\n",
       "                         'random_state': [0]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator = GradientBoostingRegressor(),\n",
    "    param_grid = parameters,\n",
    "    cv = 3,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c899f",
   "metadata": {},
   "source": [
    "- **Parâmetros testados:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0596692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>random_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.3</td>\n",
       "      <td>quantile</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.3</td>\n",
       "      <td>quantile</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.3</td>\n",
       "      <td>quantile</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.3</td>\n",
       "      <td>quantile</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.3</td>\n",
       "      <td>quantile</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate           loss  max_depth  min_samples_split  n_estimators  \\\n",
       "0              0.1  squared_error          1                  2           100   \n",
       "1              0.1  squared_error          1                  2           150   \n",
       "2              0.1  squared_error          1                  2           200   \n",
       "3              0.1  squared_error          1                  2           250   \n",
       "4              0.1  squared_error          1                  3           100   \n",
       "..             ...            ...        ...                ...           ...   \n",
       "187            0.3       quantile          2                  2           250   \n",
       "188            0.3       quantile          2                  3           100   \n",
       "189            0.3       quantile          2                  3           150   \n",
       "190            0.3       quantile          2                  3           200   \n",
       "191            0.3       quantile          2                  3           250   \n",
       "\n",
       "     random_state  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "..            ...  \n",
       "187             0  \n",
       "188             0  \n",
       "189             0  \n",
       "190             0  \n",
       "191             0  \n",
       "\n",
       "[192 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(grid_rf.cv_results_['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507875ed",
   "metadata": {},
   "source": [
    "- **Melhores parâmetros:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6fd5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor(learning_rate=0.2, max_depth=2, min_samples_split=3,\n",
      "                          n_estimators=200, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(grid_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5146a81",
   "metadata": {},
   "source": [
    "### 5. Diferença entre o Gradient Boosting e Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b877deb",
   "metadata": {},
   "source": [
    "Pensando no nome dado, a diferença está na aleatoriedade. No GBM, cada árvore de decisão é construída utilizando todo o conjunto de dados de treinamento, sem amostragem aleatória. Por outro lado, no Stochastic Gradient Boosting, cada árvore de decisão é construída utilizando uma amostra aleatória do conjunto de dados de treinamento. Essa amostragem aleatória é realizada com substituição, o que significa que uma mesma amostra pode ser selecionada várias vezes ou até mesmo não ser selecionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814cfa19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
